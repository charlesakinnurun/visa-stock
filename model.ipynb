{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31b98f74",
   "metadata": {},
   "source": [
    "Import the neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa67d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,roc_curve,auc,f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e202f0d",
   "metadata": {},
   "source": [
    "Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c1ebad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for V..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_8552\\2742512422.py:5: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(TICKER,period=\"5y\")\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Define the sticker symbol for Visa (V)\n",
    "TICKER = \"V\"\n",
    "# Dowload historical stock data for the last 5 years\n",
    "print(f\"Downloading data for {TICKER}..........\")\n",
    "data = yf.download(TICKER,period=\"5y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32845b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"visa_stock_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ca91c",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e1ae09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Missing Values =====\n",
      "Price   Ticker\n",
      "Close   V         0\n",
      "High    V         0\n",
      "Low     V         0\n",
      "Open    V         0\n",
      "Volume  V         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "data_missing = data.isnull().sum()\n",
    "print(\"===== Missing Values =====\")\n",
    "print(data_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "579af33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Duplicated Rows =====\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicated rows\n",
    "data_duplicates = data.duplicated().sum()\n",
    "print(\"===== Duplicated Rows =====\")\n",
    "print(data_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108460c3",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e70d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Target Variable: Price Movement (Classification)\n",
    "# Shift the 'Close' price one day back to get 'Next_Close'\n",
    "# This is what we want to predict: the closing price for the next day.\n",
    "data['Next_Close'] = data['Close'].shift(-1)\n",
    "# Create the binary target: 1 if Next_Close > Close (price went UP), 0 otherwise (price went DOWN/flat)\n",
    "# Use positional (numpy) comparison to avoid pandas index alignment issues when comparing Series\n",
    "data['Target'] = (data['Next_Close'].to_numpy() > data['Close'].to_numpy()).astype(int)\n",
    "\n",
    "# Drop the last row, as it will have NaN for 'Next_Close' and 'Target'\n",
    "# because we cannot know the future closing price.\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fe7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Relative Strength Index (RSI) - a momentum indicator\n",
    "def calculate_rsi(df,window=14):\n",
    "    # Calculte daily price changes\n",
    "    delta = df[\"Close\"].diff()\n",
    "    #  Separate gains (upward changes) and losses (downward changes)\n",
    "    gain = delta.where(delta > 0,0)\n",
    "    loss = -delta.where(delta < 0,0)\n",
    "\n",
    "    # Calculate the Exponential Moving Avearge (EMA) of gains and losses\n",
    "    avg_gain = gain.ewm(com=window-1,min_periods=window).mean()\n",
    "    avg_loss = loss.ewm(com=window-1,min_periods=window).mean()\n",
    "\n",
    "    # Calculate Relative Strength (RS)\n",
    "    rs = avg_gain / avg_loss\n",
    "    \n",
    "    # Calculate RS1\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Simple Moving Average (SMAs) - trend indicators\n",
    "data[\"SMA_5\"] = data[\"Close\"].rolling(window=5).mean() # 5-day Moving Average\n",
    "data[\"SMA_10\"] = data[\"Close\"].rolling(window=10).mean() # 10-day Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c226495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Relative Strength Index (RSI)\n",
    "data[\"RSI\"] = calculate_rsi(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89a8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Moving Average Convergence Divergence (MACD) - another momentum indicator\n",
    "# Typically uses 12-day EMA, 26-day EMA, and a 9-day EMA signal line\n",
    "data[\"EMA_12\"] =  data[\"Close\"].ewm(span=12,adjust=False).mean()\n",
    "data[\"EMA_26\"] = data[\"Close\"].ewm(span=26,adjust=False).mean()\n",
    "data[\"MACD\"] = data[\"EMA_12\"] - data[\"EMA_26\"]\n",
    "data[\"MACD_Signal\"] = data[\"MACD\"].ewm(span=9,adjust=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Volume as a feature, normalized by the mean value\n",
    "data[\"Volume_Norm\"] = data[\"Volume\"] / data[\"Volume\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f175e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values created by the rolling windows/EMAs (the first 26 days)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da69fa97",
   "metadata": {},
   "source": [
    "Visualization Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc0a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''print(\"Generating pre-training visualizations...........\")\n",
    "\n",
    "# Set up the plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.Figure(figsize=(14,10))\n",
    "\n",
    "# Subplot1: Closing Price Trend\n",
    "plt.subplot(2,1,1)\n",
    "data[\"Target\"].plot(title=f\"{TICKER} Stock Price Trend (5 years)\",color=\"black\",linewidth=1.5)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Closing Price (USD)\")\n",
    "plt.grid(True,linestyle=\"--\",alpha=0.6)\n",
    "\n",
    "# Subplot 2:  Target Variable Distribution (Balance Check)\n",
    "plt.subplot(2,1,2)\n",
    "data[\"Target\"].value_counts().plot(kind=\"bar\",color=[\"red\",\"blue\"])\n",
    "plt.title(\"Distribution of Target Variable (Price Movement)\")\n",
    "plt.xticks([0,1],[\"Down/Flat (0)\",\"Up (1)\"])\n",
    "plt.ylabel(\"Count of Days\")\n",
    "plt.grid(axis=\"y\",linestyle=\"--\",alpha=0.6)\n",
    "plt.tight_layout(rect=[0,0.03,1,0.95])\n",
    "plt.suptitle(\"Pre-Training Data Analysis\",fontsize=16,fontweight=\"bold\")\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b374fa",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f551bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features (X) to be used for training. Exclude the original 'Open', 'High', 'Low', 'Close', 'Adj Close'\n",
    "FEATUERES = [\"SMA_5\",\"SMA_10\",\"RSI\",\"MACD\",\"MACD_Signal\",\"Volume_Norm\"]\n",
    "X = data[FEATUERES]\n",
    "y = data[\"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f539c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 15)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373375b",
   "metadata": {},
   "source": [
    "Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c7c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (70%) and testing (30%) sets\n",
    "# shuffle=False is cruical for time series data to maintain chronological order\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2626336",
   "metadata": {},
   "source": [
    "Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca7b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler (Standardization)\n",
    "# Scaling is essential for models sensitive to feature magnitudes, like LogisticRegression and SVM\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler only on the training data to prevent data leakage\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the fitted scaler to both training and test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b42a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the scaled arrays back to DataFrames for easier handling (optional but good practice)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled,columns=FEATUERES,index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled,columns=FEATUERES,index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2364145",
   "metadata": {},
   "source": [
    "Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d93439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary of classifiers to compare\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "    \"Support Vector Machine\": SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\n--- Model Training and Comparison ---\")\n",
    "# Loop through each classifier, train it, evaluate performance, and store results\n",
    "for name, model in classifiers.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    # Train the model using the scaled training data\n",
    "    if name in [\"Logistic Regression\", \"Support Vector Machine\"]:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        # Make predictions on the scaled test data\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        # Tree-based models and Naive Bayes are less sensitive to scaling, but we use scaled data for consistency\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the F1-Score (often better than Accuracy for financial data which may be imbalanced)\n",
    "    results[name] = {'Accuracy': acc, 'F1-Score': f1}\n",
    "    print(f\"{name} - F1 Score: {f1:.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Convert results dictionary to a DataFrame for easy visualization\n",
    "results_df = pd.DataFrame(results).T.sort_values(by='F1-Score', ascending=False)\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d70c38",
   "metadata": {},
   "source": [
    "Visualization After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a5c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison of the models' performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=results_df.index, y='F1-Score', data=results_df, palette='viridis')\n",
    "plt.title('Classification Model F1-Score Comparison', fontsize=14)\n",
    "plt.ylabel('F1-Score')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e198dac8",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning on the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaab685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Random Forest as the target for tuning, as it is robust and often performs well\n",
    "best_model_name = \"Random Forest\"\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid to search through\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],         # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20],            # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5],            # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2]              # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV. Scoring is set to 'f1' since we prioritize balanced performance.\n",
    "# We use the unscaled data for tree-based models\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='f1', verbose=1, n_jobs=-1)\n",
    "\n",
    "print(f\"\\n--- Hyperparameter Tuning using GridSearchCV on {best_model_name} ---\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "print(f\"Best F1 Score on training data (CV): {best_score:.4f}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "tuned_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the tuned model on the test set\n",
    "y_pred_tuned = tuned_model.predict(X_test)\n",
    "y_proba_tuned = tuned_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Final Metrics\n",
    "final_f1 = f1_score(y_test, y_pred_tuned)\n",
    "final_acc = accuracy_score(y_test, y_pred_tuned)\n",
    "print(f\"\\n--- Tuned {best_model_name} Final Evaluation ---\")\n",
    "print(f\"Test F1 Score: {final_f1:.4f}\")\n",
    "print(f\"Test Accuracy: {final_acc:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e3b03",
   "metadata": {},
   "source": [
    "Final Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec08e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve components\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba_tuned)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1: ROC Curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title(f'Receiver Operating Characteristic (ROC) - Tuned {best_model_name}', fontsize=12)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Subplot 2: Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_tuned)\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Predicted Down', 'Predicted Up'],\n",
    "            yticklabels=['Actual Down', 'Actual Up'])\n",
    "plt.title(f'Confusion Matrix - Tuned {best_model_name}', fontsize=12)\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.suptitle('Post-Training Model Evaluation', fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7969f1",
   "metadata": {},
   "source": [
    "Prediction Input for New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d8b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make a prediction for a new day\n",
    "def predict_new_day(model, scaler, new_data_dict, feature_list):\n",
    "    \"\"\"\n",
    "    Takes a dictionary of new data, scales it, and makes a prediction.\n",
    "    NOTE: In a real-world scenario, you would need to calculate the \n",
    "    technical indicators for the new day based on the past N days of data.\n",
    "    This function simulates the input features being pre-calculated.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Simulating New Day Prediction ---\")\n",
    "\n",
    "    # Convert the input dictionary to a DataFrame\n",
    "    new_df = pd.DataFrame([new_data_dict], columns=feature_list)\n",
    "    \n",
    "    # Determine if the model needs scaling (Logistic Regression, SVM)\n",
    "    if isinstance(model, (LogisticRegression, SVC)):\n",
    "        # Scale the new data using the fitted scaler (CRITICAL: DO NOT refit)\n",
    "        new_data_scaled = scaler.transform(new_df)\n",
    "        prediction_input = new_data_scaled\n",
    "        print(\"Data scaled before prediction.\")\n",
    "    else:\n",
    "        # Use unscaled data for tree-based models\n",
    "        prediction_input = new_df\n",
    "        print(\"Using unscaled data for tree model prediction.\")\n",
    "    \n",
    "    # Make the prediction (0 or 1)\n",
    "    prediction = model.predict(prediction_input)[0]\n",
    "    \n",
    "    # Try to get the probability if the model supports it\n",
    "    try:\n",
    "        probability = model.predict_proba(prediction_input)[0]\n",
    "        prob_up = probability[1]\n",
    "        \n",
    "        result_text = \"UP (1)\" if prediction == 1 else \"DOWN/FLAT (0)\"\n",
    "        \n",
    "        print(f\"Raw Input Features: {new_data_dict}\")\n",
    "        print(f\"Predicted Outcome: {result_text}\")\n",
    "        print(f\"Probability of Price UP (1): {prob_up*100:.2f}%\")\n",
    "        \n",
    "    except AttributeError:\n",
    "        # Some models (like standard SVC without probability=True) don't have predict_proba\n",
    "        result_text = \"UP (1)\" if prediction == 1 else \"DOWN/FLAT (0)\"\n",
    "        print(f\"Predicted Outcome: {result_text}\")\n",
    "        print(\"Probability estimate not available for this model.\")\n",
    "        \n",
    "    return prediction\n",
    "\n",
    "# --- Example of New Prediction Input ---\n",
    "# These values are based on the expected input features:\n",
    "# ['SMA_5', 'SMA_10', 'RSI', 'MACD', 'MACD_Signal', 'Volume_Norm']\n",
    "\n",
    "# Simulate new data for the next day (all features must be provided)\n",
    "simulated_new_data = {\n",
    "    'SMA_5': 200.5,           # Current 5-day moving average\n",
    "    'SMA_10': 199.8,          # Current 10-day moving average\n",
    "    'RSI': 55.0,              # Current Relative Strength Index (above 50 suggests momentum)\n",
    "    'MACD': 0.5,              # Current MACD value (positive suggests bullish momentum)\n",
    "    'MACD_Signal': 0.4,       # Current MACD Signal Line\n",
    "    'Volume_Norm': 1.1        # Current Volume (10% higher than average)\n",
    "}\n",
    "\n",
    "# Run the prediction using the final, tuned Random Forest model\n",
    "final_prediction = predict_new_day(tuned_model, scaler, simulated_new_data, FEATURES)\n",
    "# The prediction result is now stored in the final_prediction variable\n",
    "print(f\"The model's final binary prediction for {TICKER} tomorrow is: {final_prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
